{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66294ff8-3933-44f8-8337-3df9397a21c5",
   "metadata": {},
   "source": [
    "Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef1f8e-4ab1-4f59-91dd-480ec0b64280",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups and determine if there are significant differences among them. To use ANOVA effectively and interpret the results accurately, certain assumptions must be met. Violations of these assumptions can impact the validity of the ANOVA results. The key assumptions for ANOVA are:\n",
    "\n",
    "Independence: The observations within each group or treatment level are assumed to be independent of each other. Violations of independence can occur when there is dependency or correlation among the observations, such as in repeated measures or clustered data. For example, if the measurements taken from individuals within a group are correlated, it violates the independence assumption.\n",
    "\n",
    "Normality: The data within each group should follow a normal distribution. This assumption is necessary for accurate hypothesis testing and confidence interval estimation. Violations of normality can occur when the data deviate significantly from a normal distribution. This can happen if the data is highly skewed or has heavy tails. For example, if the residuals of the ANOVA model do not follow a normal distribution, it violates the assumption of normality.\n",
    "\n",
    "Homogeneity of variance (homoscedasticity): The variability of the data within each group should be approximately equal. Homoscedasticity assumes that the spread of the data points is similar across all treatment levels. Violations of homogeneity of variance can occur when the variability differs significantly among the groups. This is known as heteroscedasticity. For example, if the variances of the residuals are different across the groups, it violates the assumption of homogeneity of variance.\n",
    "\n",
    "Independence of errors: The errors or residuals should be independent of each other and have no systematic patterns. Violations of independence of errors can occur when there is autocorrelation or when errors are correlated in some way. For example, if the residuals from one observation are correlated with the residuals from neighboring observations, it violates the assumption of independence of errors.\n",
    "\n",
    "Violations of these assumptions can affect the validity and reliability of the ANOVA results. It is important to check the assumptions before applying ANOVA and consider alternative statistical methods if the assumptions are violated. There are also robust versions of ANOVA that are more tolerant to violations of the assumptions, but their applicability depends on the specific situation and the nature of the violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f765e-3351-4778-8c69-5223d18512b6",
   "metadata": {},
   "source": [
    "Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7efdfa-cf37-4da8-bb27-0cff5da6e8d2",
   "metadata": {},
   "source": [
    "One-Way ANOVA: One-Way ANOVA is used when there is one categorical independent variable (also known as a factor) with three or more levels, and a continuous dependent variable. It is used to determine if there are significant differences in the means of the dependent variable across the levels of the independent variable. One-Way ANOVA is appropriate when you want to compare the means of three or more groups. For example, you might use One-Way ANOVA to compare the average test scores of students from different schools.\n",
    "\n",
    "Two-Way ANOVA: Two-Way ANOVA is used when there are two categorical independent variables (factors) and one continuous dependent variable. It allows you to examine the main effects of each independent variable as well as the interaction between the variables. Two-Way ANOVA is suitable when you want to investigate the effects of two independent variables on a dependent variable. For example, you might use Two-Way ANOVA to analyze the effects of both gender and treatment type on patient outcomes.\n",
    "\n",
    "Three-Way ANOVA: Three-Way ANOVA is used when there are three categorical independent variables (factors) and one continuous dependent variable. It extends the analysis to three independent variables and their interactions. Three-Way ANOVA is applicable when you want to examine the effects of three independent variables on a dependent variable. For example, you might use Three-Way ANOVA to analyze the effects of age, gender, and education level on income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4651c5-7e5b-4f28-aeaa-550d818f82c5",
   "metadata": {},
   "source": [
    "Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f52e1-bd57-4437-b466-ed3c8985a0ff",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the decomposition of the total variance in the data into different components based on the sources of variation. It allows us to understand the relative contributions of different factors and their interactions to the overall variability observed in the dependent variable. The partitioning of variance is essential in ANOVA because it helps us:\n",
    "\n",
    "Identify significant sources of variation: By decomposing the total variance into different components, ANOVA enables us to determine which factors are contributing significantly to the variation in the dependent variable. This information helps us identify the main effects of individual factors and potential interactions between them.\n",
    "\n",
    "Assess the significance of effects: ANOVA provides a statistical framework to test the null hypothesis that there are no significant differences among the group means. The partitioning of variance allows us to calculate the variability between groups (due to the factors) and within groups (due to random variation). By comparing these variances and performing hypothesis tests, we can assess the significance of the effects and determine if there are significant differences among the groups.\n",
    "\n",
    "Quantify the magnitude of effects: The partitioning of variance provides an estimation of the magnitude of the effects. By calculating the proportion of variance explained by each factor and their interactions, ANOVA allows us to understand the relative importance of different factors in explaining the variation in the dependent variable. This information helps in interpreting the practical significance of the effects observed.\n",
    "\n",
    "Guide further analysis: Understanding the partitioning of variance guides further analysis, such as post-hoc tests or planned comparisons. By identifying significant factors or interactions, we can perform additional tests to explore specific group differences and understand the nature of the effects.\n",
    "\n",
    "Overall, the partitioning of variance in ANOVA is crucial for understanding the factors contributing to variation in the data, assessing significance, quantifying effect sizes, and guiding further analysis. It provides a systematic approach to examine the relationships between independent variables and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d09788-63f6-44ac-9a60-c44bb4eb431e",
   "metadata": {},
   "source": [
    "Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d364c-6903-4ef3-b7a6-f4efae26e667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45fb59-cdbb-4f5f-bb53-6c5ac79b5516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7cc09f-e2be-4f6e-b0b5-41d0f8de4f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 59, 76, 40, 72, 3, 69, 64, 51, 81]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "data = []\n",
    "for _ in range(10):\n",
    "    random_number = random.randint(0, 100)\n",
    "    data.append(random_number)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30f34d4-21e3-48d6-a61d-0422fdad3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\"A\", \"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c45ebf-559c-4287-86c2-08a0ce52047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3444.0 1600.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "overall_mean = np.mean([data])\n",
    "SST = np.sum((data - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the group means\n",
    "group_means = []\n",
    "for group in np.unique(groups):\n",
    "    group_data = data[groups == group]\n",
    "    group_mean = np.mean(group_data)\n",
    "    group_means.append(group_mean)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "SSE = np.sum((group_means - overall_mean) ** 2) * len(np.unique(groups))\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(SSR, SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325a47f-c523-46c0-9ba8-51fd3912c183",
   "metadata": {},
   "source": [
    "Q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a3bcca-8ffd-4048-a884-ef0a4b0d4243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.479e+30\n",
      "Date:                Thu, 04 Jan 2024   Prob (F-statistic):           6.76e-31\n",
      "Time:                        10:31:22   Log-Likelihood:                 193.74\n",
      "No. Observations:                   6   AIC:                            -379.5\n",
      "Df Residuals:                       2   BIC:                            -380.3\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -2.0000   1.36e-14  -1.47e+14      0.000      -2.000      -2.000\n",
      "X1             4.0000   6.28e-15   6.37e+14      0.000       4.000       4.000\n",
      "X2             2.0000   8.58e-15   2.33e+14      0.000       2.000       2.000\n",
      "X1:X2       4.441e-15   3.97e-15      1.118      0.380   -1.26e-14    2.15e-14\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.000\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.562\n",
      "Skew:                           0.000   Prob(JB):                        0.755\n",
      "Kurtosis:                       1.500   Cond. No.                         46.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 6 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with the data\n",
    "data = pd.DataFrame({'X1': [1, 1, 2, 2, 3, 3],\n",
    "                     'X2': [1, 2, 1, 2, 1, 2],\n",
    "                     'Y': [4, 6, 8, 10, 12, 14]})\n",
    "\n",
    "# Create a model\n",
    "model = sm.formula.ols('Y ~ X1+ X2 + X1*X2', data=data)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Get the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898786b-40ae-4dc2-ab75-6c42027f6c91",
   "metadata": {},
   "source": [
    "Q6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ab3bd-a133-45f7-8f85-7f0333fe3db6",
   "metadata": {},
   "source": [
    "In the given scenario, conducting a one-way ANOVA resulted in an F-statistic of 5.23 and a p-value of 0.02. Based on these results, we can draw the following conclusions:\n",
    "\n",
    "There are significant differences between the groups: The obtained F-statistic of 5.23 indicates that there is variability between the groups that is larger than the variability within the groups. This suggests that there are statistically significant differences in at least one pair of group means.\n",
    "\n",
    "p-value is less than the chosen significance level (commonly set at 0.05): With a p-value of 0.02, which is smaller than 0.05, we have evidence to reject the null hypothesis. The null hypothesis assumes that there are no significant differences between the groups. Since the p-value is below the significance level, we can conclude that the differences observed are statistically significant.\n",
    "\n",
    "Further investigation is required to identify specific group differences: Although the one-way ANOVA provides evidence of overall group differences, it does not specify which particular groups are different from each other. To determine the specific group(s) that differ, post hoc tests or pairwise comparisons can be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9849eb-aead-406e-8779-86550ab474bc",
   "metadata": {},
   "source": [
    "Q7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdac0c2-2c75-4670-ab5b-48033e8c7f15",
   "metadata": {},
   "source": [
    "In a repeated measures ANOVA, missing data can pose challenges as it may lead to biased or inefficient estimates if not properly handled. Here are some approaches to handle missing data in a repeated measures ANOVA:\n",
    "\n",
    "Complete Case Analysis: One approach is to exclude any cases with missing data from the analysis, analyzing only the complete cases. This method is straightforward but may result in reduced sample size and potential bias if the missing data is related to the variables under study.\n",
    "\n",
    "Pairwise Deletion: With pairwise deletion, missing data are handled on a variable-by-variable basis. Each analysis involves using only the available data for that particular variable, discarding cases with missing data for other variables. This approach retains more cases for analysis but can introduce bias if the missing data is related to the variables.\n",
    "\n",
    "Mean Substitution: Another simple approach is to substitute missing values with the mean of the available data for that variable. This method assumes that the missing values are missing completely at random (MCAR). However, mean substitution can lead to underestimation of variances and may not accurately reflect the true variability.\n",
    "\n",
    "Multiple Imputation: Multiple imputation involves estimating missing values based on the observed data and incorporating uncertainty. It generates multiple plausible imputations, creating complete datasets. The analysis is then performed on each imputed dataset, and the results are pooled. This approach accounts for the uncertainty of missing values, preserves the sample size, and provides unbiased estimates if the missingness is ignorable.\n",
    "\n",
    "The consequences of using different methods to handle missing data can vary:\n",
    "\n",
    "Complete case analysis and pairwise deletion can lead to biased estimates and reduced statistical power if the missing data is related to the variables of interest. These methods assume missingness to be completely random, which may not be realistic.\n",
    "\n",
    "Mean substitution, while easy to implement, can distort the relationships and variability among variables and may not capture the true nature of the missing data.\n",
    "\n",
    "Multiple imputation, if implemented appropriately, can provide more reliable and valid estimates by accounting for the uncertainty introduced by missing data. However, the quality of imputation depends on the assumptions made and the imputation model used.\n",
    "\n",
    "Choosing the appropriate method for handling missing data in a repeated measures ANOVA depends on the nature of the missing data, the assumptions made, and the overall research objectives. It is essential to carefully consider the potential biases and limitations associated with each approach and select the most suitable method based on the specific context and characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11283002-fc69-4f5a-bb57-b3a685206d5f",
   "metadata": {},
   "source": [
    "Q8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453ab1b-e402-4d08-85f5-10e42e4e2798",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are typically performed to determine which specific group differences are responsible for the observed significance. Here are some commonly used post-hoc tests:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD): Tukey's HSD test compares all possible pairs of group means and provides simultaneous confidence intervals to identify significant differences. This test is appropriate when you have equal group sizes and homogeneous variances. It controls the familywise error rate, making it suitable for multiple comparisons.\n",
    "\n",
    "Bonferroni Correction: The Bonferroni correction adjusts the significance level to account for multiple comparisons. It divides the desired alpha level by the number of comparisons to maintain the overall Type I error rate. This method is more conservative, but it effectively controls the familywise error rate.\n",
    "\n",
    "Scheffe's Test: Scheffe's test is a conservative post-hoc test that allows for comparisons between groups while controlling the overall familywise error rate. It is robust and can be used when the groups have unequal sizes and variances.\n",
    "\n",
    "Dunnett's Test: Dunnett's test compares several treatment groups against a control group. It controls the overall Type I error rate while accounting for multiple comparisons against a single reference group. This test is suitable when there is a control group to compare against multiple treatment groups.\n",
    "\n",
    "Games-Howell Test: The Games-Howell test is a non-parametric alternative to post-hoc tests when the assumptions of equal variances and normality are violated. It allows for unequal group sizes and variances and does not assume homogeneous variances.\n",
    "\n",
    "Example situation:\n",
    "\n",
    "Let's say a researcher conducted a study to compare the effectiveness of four different teaching methods (A, B, C, and D) on students' test scores. They performed an ANOVA and found a significant overall effect. In this case, a post-hoc test would be necessary to determine which teaching methods significantly differ from each other.\n",
    "\n",
    "They could use Tukey's HSD test to conduct pairwise comparisons and identify the specific group differences. The test would provide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d1665-474a-49db-bf05-23729d50ee07",
   "metadata": {},
   "source": [
    "Q9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64a8eb-cd09-416f-93c3-cf3df2a20dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Weight loss data for each diet group\n",
    "diet_A = [2.3, 3.1, 1.9, 2.8, 2.5, 3.2, 2.9, 2.6, 3.5, 2.4,\n",
    "          2.7, 2.1, 3.0, 3.3, 2.2, 2.6, 3.1, 2.8, 2.5, 2.9,\n",
    "          2.7, 2.3, 3.2, 2.4, 2.1, 2.6, 2.8, 2.7, 2.9, 2.2,\n",
    "          2.3, 2.1, 2.8, 3.0, 2.9, 2.7, 2.4, 2.6, 2.2, 2.5,\n",
    "          2.3, 2.7, 2.9, 2.6, 2.8, 3.1, 2.4, 2.5, 2.2, 2.3]\n",
    "\n",
    "diet_B = [3.9, 3.6, 4.1, 3.2, 4.3, 3.8, 4.2, 3.7, 4.0, 3.5,\n",
    "          3.9, 4.1, 3.6, 4.0, 3.7, 3.5, 3.9, 4.2, 3.6, 4.1,\n",
    "          3.7, 4.0, 3.8, 4.2, 3.6, 4.0, 3.9, 4.1, 3.7, 4.3,\n",
    "          3.6, 4.0, 3.5, 3.9, 3.7, 4.1, 3.6, 4.3, 3.8, 4.2,\n",
    "          3.9, 3.5, 4.0, 3.7, 4.1, 3.6, 4.3, 3.8, 4.2, 3.9]\n",
    "\n",
    "diet_C = [2.7, 1.8, 2.9, 2.4, 2.1, 2.8, 2.5, 2.9, 2.6, 2.3,\n",
    "          2.4, 2.1, 2.7, 1.9, 2.6, 2.8, 2.5, 2.9, 2.4, 2.1,\n",
    "          2.7, 2.3, 2.8, 2.6, 2.9, 2.4, 2.1, 2.7, 2.3, 2.9,\n",
    "          2.8, 2.6, 2.4, 2.7, 2.3, 2.9, 2.6, 2.4, 2.1, 2.7,\n",
    "          2.3, 2.9, 2.8, 2.6, 2.4, 2.1, 2.7, 2.3, 2.9, 2.6]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a0dda-377a-4999-8194-718321709c06",
   "metadata": {},
   "source": [
    "Q10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995a70c-03b7-4200-b339-81d59ae13b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "data = pd.DataFrame({\n",
    "    'Program': ['A', 'B', 'C'] * 10,\n",
    "    'Experience': ['Novice'] * 15 + ['Experienced'] * 15,\n",
    "    'Time': [10, 12, 11, 14, 15, 13, 9, 10, 11, 12,\n",
    "             14, 15, 13, 10, 12, 11, 14, 15, 13, 9,\n",
    "             10, 11, 12, 14, 15, 13, 10, 12, 11, 14]\n",
    "})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ C(Program) + C(Experience) + C(Program):C(Experience)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the results\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084e70e-8326-48aa-9d7b-7c872ecd88ff",
   "metadata": {},
   "source": [
    "Q11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0345b9-2315-489e-8860-eccb42efefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Test scores for the control group\n",
    "control_scores = np.array([78, 82, 85, 90, 88, 75, 79, 84, 92, 80,\n",
    "                          87, 81, 79, 83, 86, 89, 80, 77, 84, 78,\n",
    "                          81, 85, 88, 82, 76, 83, 80, 87, 79, 81,\n",
    "                          84, 90, 88, 85, 79, 82, 87, 83, 81, 86,\n",
    "                          78, 84, 89, 88, 82, 80, 85, 86, 83, 79])\n",
    "\n",
    "# Test scores for the experimental group\n",
    "experimental_scores = np.array([84, 87, 90, 92, 95, 81, 85, 89, 93, 88,\n",
    "                               92, 86, 83, 90, 91, 94, 85, 80, 88, 82,\n",
    "                               86, 91, 94, 88, 84, 85, 83, 89, 85, 87,\n",
    "                               92, 94, 91, 89, 86, 85, 90, 87, 85, 92,\n",
    "                               84, 90, 93, 94, 87, 85, 89, 92, 86, 84])\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369a6b5-361b-4443-9775-9623110f5b43",
   "metadata": {},
   "source": [
    "Q12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9854a0-f07b-4b8f-af16-4ceb86e2259f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
